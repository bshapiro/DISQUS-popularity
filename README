This is a small project (written in about ~5 hours) built for a day-long interview at DISQUS, Inc (http://www.disqus.com). 



Some notes on this project:

1. This project allows the user to input the name of a DISQUS forum, generates the most popular terms in the forum, and then ranks a comment provided by the user as more or less popular. Note that clicking the final button does not produce a visible result; due to time restrictions, I did not have a chance to get some of the last steps done. However, a ranking is logged in the console when the user clicks "How popular is my comment?". 
  
2. Some notes on the ranking provided. This ranking is produced with a slightly cut-down version of the Naive Bayes algorithm. The Naive Bayes algorithm assumes conditional independence of all words given a category (in this case, our category is popular vs. unpopular). So, the probability of a comment given it being in the popular category is:

  p(w1|popular)*p(w2|popular) * ... * p(wn|popular)

where wn is the nth word in the comment. Naive Bayes says that:

  p(popular|document) is proportional to p(document|popular)*p(popular)

I cut this down slightly because given our current data, there is really only one category: popular. I had no data to classify as "not popular" since "dislikes" aren't really a thing anyone seems to use very much in the Disqus system! So the only thing I compute is the probability of a comment given that it is popular. This means that the probability of each word is simply the number of times that word has showed up in all of the popular comments divided by the number of words in the popular comments. This gives us:

P(comment) = appearances of w1 / total words * appeareances of w2/total words * ... etc. 

Some further details on this. I did not use the above equation because for real-life applications of naive Bayes, this kind of computation is intractable. Given 20,000 words and a comment with some words that appear 400 times each (still very significant), the probability of a comment still ends up being infinitesimally small; essentially 0. To avoid this, I resort to a logarithmic scale, so that the probability of a word given that it is popular is:

log(number of times the word appears in the popular comments) - log(total words)

This results in a logarithmic scale of negative numbers -- rankings closer to 0 mean comments are more popular, while more negative means less popular. Note again that this is a logarithmic scale, so differences are not linear; they are exponential. A difference of 6 between two comments (say the first scores -14 and the second scores -8) is not as small as one might normally think... This is actually very significant.

To avoid 0 proabilities due to words appearing in the user input that have not been found in the comments database, I use Laplace smoothing (also known as add-one smoothing). 

3. A final note on the ranking. I am giving this one its own number because it's fairly important! :-) There is a scalability problem that I was not able to fix given the time constraints. Larger comments will naturally be less probable than smaller comments. With the standard naive Bayes algorithm, this is not an issue since you merely compare the same comment over many categories. Here, we only have one category, so probabilities need to somehow be scaled depending on the size of the comment. Therefore, PLEASE note: when testing, use only comments of the same length to check the computation. For instance:

"I think that people need a government, and that people need Obama." 

would receive a much higher ranking on cnn than:

"I think that kittens like to fly, and that they are purple."

Note that I used the same amount of words. If you try:

"I think that people need a government, and that people need Obama."

and

"I like cats."

then the latter will naturally get a much higher ranking because the results have not been normalized relative to comment length. 
